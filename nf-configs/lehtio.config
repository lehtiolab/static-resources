params {
  overbook_cpus_factor = 2
}

executor {
  name = 'slurm'
}

docker {
  enabled = true
  fixOwnership = true
}

singularity {
  enabled = false
}
conda.enabled = false


// Kantele lib files
data_file_path = "/mnt/lehtio-rawstorage/storage/databases"

// For storing big stuff like reference data
bigdata_file_path = '/mnt/lehtio-rawstorage/reference'


def listify(it) {
  /* This function is useful when needing a list even when having a single item
  - Single items in channels get unpacked from a list
  - Processes expect lists. And here the config for percolator needs a list.
  See:
  https://github.com/nextflow-io/nextflow/discussions/4240

  This function is also in pipelines sometimes, and that will override the definition here
  */
  return it instanceof java.util.List ? it : [it]
}


process {
  withName: get_software_versions {
    errorStrategy = { task.exitStatus == 141 ? 'retry' : 'finish' }
    maxRetries = 5
  }
  withName: complementSpectraLookupCleanPSMs {
    scratch = true
  }
  withName: createNewSpectraLookup {
    time = { mzmlfiles.size() > 100 ? check_max( 0.02.h * mzmlfiles.size() * task.attempt, 'time' ) : 2.h * task.attempt}
    scratch = true
  }
  withName: quantifySpectra {
    maxForks = 10
    memory = { (infile.size() >> 30) > 5 ? check_max(infile.size() * 4 as nextflow.util.MemoryUnit, 'memory') : '16 GB'}
    time = { check_max( 1.h * (infile.size() >> 30) * task.attempt, 'time' ) }
  }
  withName: quantifyMS1 {
    cpus = 4
  }
  withName: msgfPlus {
    maxForks = 20
    cpus = 4
    memory = { (db.size() >> 30) < 1 ? 16.GB : "${db.size() * 16}B"  }
    time = { (mzml.size() >> 30) < 1 ? check_max(3.h * task.attempt, 'time') : check_max((3 + (mzml.size() >> 30)) * task.attempt * 1.h, 'time') }
  }
  withName: quantLookup {
    time = { check_max( 0.1.h * isofns.size() * task.attempt, 'time' ) }
    memory = '32 GB'
    scratch = true
  }
  withName: createPSMTable {
    // lookup size is a proxy for amount of PSMs
    time = { (lookup.size() >> 30) > 1 ? check_max( 1.h * (lookup.size() >> 30) * task.attempt, 'time' ) : 2.h * task.attempt }
    // protein grouping needs memory if there's lots of information
    memory = { (lookup.size() >> 30) < 40 ? task.attempt * 16.GB : check_max("${(task.attempt * lookup.size() * 0.4).round()}B", 'memory') }
    scratch = true
  }
  withName: percolator {
    time = { listify(mzids).size() > 40 ? check_max( 0.1.h * listify(mzids).size() * task.attempt , 'time' ) : 4.h * task.attempt }
    cpus = 4
  }
  withName: DEqMS {
    cpus = 10
  }
  withName: proteinPeptideSetMerge {
    scratch = true
    // lookup size is a proxy for amount of PSMs
    time = { (lookup.size() >> 30) > 1 ? 1.h * (lookup.size() >> 30) * task.attempt : 2.h * task.attempt }
  }

  withName: luciphorPTMLocalizationScoring {
    time = { check_max( (allpsms.size() >> 20) * 0.0067.h * task.attempt, 'time') }
    cpus = 2
  }

  
  withName: countMS2perFile {
    scratch = true
  }
  withName: createPTMLookup {
    scratch = true
  }
  withName: createPTMTable {
    scratch = true
  }
  withName: mergePTMPeps {
    scratch = true
  }
  withName: createTrypticMatchDB {
    scratch = true
  }
  withName: markPeptidesPresentInDB {
    scratch = true
  }
  withName: joinAnnotatedSeqmatchPeptides {
    scratch = true
  }

  // labelcheck
  withName: peptidesProteinsReport {
    scratch = true
  }

  // iPAW
  withName: makeTargetSeqLookup {
    scratch = true
  }
  withName: concatFasta {
    scratch = true
  }
  withName: filterPercolator {
    scratch = true
  }
  withName: labelnsSNP {
    scratch = true
  }

  // Todo - for this scratch - USE TAGS?
  // PGT
  withName: prepareFilterDB {
    // also iPAW
    scratch = true
  }
  withName: createDecoy {
    scratch = true
  }
  withName: getVariantPrefixes {
    scratch = true
  }
  withName: splitAndFilterPercolator {
    scratch = true
  }
  withName: PSMQC {
    scratch = true
  }
  withName: mergePeptideTables {
    scratch = true
  }
  withName: joinPeptidePis {
    scratch = true
  }
  withName: referencePiToSqlite {
    scratch = true
  }
  withName: msstitchCreateFullProteinDB {
    scratch = true
  }
  withName: postSpecAIJoin {
    scratch = true
  }
  withName: concatPSMSetsLoadDB {
    scratch = true
  }
  withName: getDBPeptidesForTable {
    scratch = true
  }
  withName: writeAllPeptides {
    scratch = true
  }
  withName: writeAllPeptidesFasta {
    scratch = true
  }
  withName: filterIrrelevantNovelSequencesPSMs {
    scratch = true
  }
  withName: mergeSetsPepProtTypes {
    scratch = true
  }
  withName: extractProteinPepseqsFromDB {
    scratch = true
  }
  withName: summaryDB {
    scratch = true
  }
  withName: countScans {
    scratch = true
  }
  withName: sqliteInit {
    scratch = true
  }
  withName: gffUtilsDB {
    scratch = true
  }
  withName: addIntronsUpDownstreamGTF {
    scratch = true
  }
  withName: vcfToProtSeq {
    scratch = true
  }
  withName: insertPeptidesDB {
    scratch = true
  }
  withName: summaryReport {
    // no sqlite but depending on symlinks on share that are directories and those
    // do not work (os.walk thinks they are files) on our share since they are mfsymlinks over samba
    scratch = true
  }

  withName: mzRefine {
    // network writes sometimes dont work?
    scratch = true
  }
  withName: msconvert {
    // network writes sometimes dont work?
    scratch = true
    time = 5.h
  }
  withName: centroidMS1 {
    // network writes sometimes dont work?
    scratch = true
  }
}
